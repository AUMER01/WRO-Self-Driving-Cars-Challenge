ğŸ¤– AUMers â€“ AUM University Future Engineers 2025
<p align="center"> <img width="512" height="512" alt="AUMers Logo" src="https://github.com/user-attachments/assets/88de7538-2cd9-4d02-96d0-4ed30c3b0e1c" /> </p>
ğŸ“˜ Table of Contents

Project Overview

Repository Structure

docs/

code/

main_controller/

vision/

utils/

models/

videos/

photos/

Hardware and Components

Software Description

Mobility, Power, and Sensing System

Obstacle Management and Vision System

Testing and Calibration

Results and Performance

Video Demonstration

Team Members

License

ğŸ§© Project Overview

AUMers is an autonomous vehicle developed by AUM University (Kuwait) for the World Robot Olympiad â€“ Future Engineers 2025 competition.
The vehicle is designed to navigate a track autonomously, complete three laps under randomized wall conditions, detect colored traffic signs (red and green pillars), and finish with an automatic parking maneuver.

This project demonstrates the integration of mobility control, sensor-based navigation, and computer vision using an Xmotion controller, ultrasonic sensors, and a PixyCam.

ğŸ—‚ï¸ Repository Structure
AUM-Future-Engineers-2025/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Vehicle_Design_Report.pdf
â”‚   â”œâ”€â”€ System_Architecture.png
â”‚   â”œâ”€â”€ Sensor_Placement_Diagram.png
â”‚   â””â”€â”€ commit_log_summary.txt
â”‚
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ main_controller/
â”‚   â”‚   â”œâ”€â”€ main.py / main.cpp
â”‚   â”‚   â”œâ”€â”€ motor_control.py
â”‚   â”‚   â”œâ”€â”€ navigation.py
â”‚   â”‚   â””â”€â”€ sensors.py
â”‚   â”œâ”€â”€ vision/
â”‚   â”‚   â”œâ”€â”€ line_tracking.py
â”‚   â”‚   â””â”€â”€ object_detection.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ helper_functions.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ chassis_3d_model.stl
â”‚   â”œâ”€â”€ sensor_mount.stl
â”‚   â””â”€â”€ wheel_cad_model.stl
â”‚
â”œâ”€â”€ videos/
â”‚   â””â”€â”€ demo_link.txt
â”‚
â””â”€â”€ photos/
    â”œâ”€â”€ top_view.jpg
    â”œâ”€â”€ bottom_view.jpg
    â”œâ”€â”€ left_side.jpg
    â”œâ”€â”€ right_side.jpg
    â”œâ”€â”€ front_view.jpg
    â”œâ”€â”€ rear_view.jpg
    â””â”€â”€ team_photo.jpg

ğŸ“ docs/

Contains all reference documents and diagrams related to system design, sensor placement, and commit history.

âš™ï¸ code/

Includes all software used to control and operate the robot:

main_controller/ â€“ core driving and logic code

vision/ â€“ image processing and traffic sign recognition

utils/ â€“ helper functions shared between modules

ğŸ§± models/

3D printable and CAD files used for the robotâ€™s mechanical structure.

ğŸ¥ videos/

Contains a text file with a YouTube demo link showing the autonomous driving test.

ğŸ“¸ photos/

Includes high-resolution images of the robot (all sides), system components, and the team.

ğŸ”© Hardware and Components
Component	Description	Purpose
Xmotion Controller	Integrated motor control + logic board	Core control and communication unit
DC Motors	Dual drive motors	Provide motion and steering control
Servo Motor	Steering actuator	Front wheel steering
Ultrasonic Sensors (HC-SR04)	3 units (front, left, right)	Distance measurement and wall alignment
PixyCam	Vision sensor	Detects traffic signs and parking cues
7.4V Li-Po Battery	Power supply	Provides energy to the system
3D Printed Chassis and Mounts	Custom design	Mechanical support and sensor positioning
ğŸ’» Software Description

The software is written in Python and runs on the Xmotion platform.
The architecture is divided into independent modules for sensing, control, navigation, and vision.
Key algorithms include:

PID-based motor speed control

Obstacle avoidance logic using ultrasonic data

Vision-based color detection for lane side selection

Parallel parking maneuver using combined vision and distance feedback

ğŸš— Mobility, Power, and Sensing System

The robotâ€™s mobility relies on two DC motors controlled through the Xmotion board, providing smooth and precise motion.
A 7.4V Li-Po battery powers all components efficiently.
Three ultrasonic sensors maintain the robotâ€™s centered position by comparing left and right distances, ensuring lane balance and wall avoidance.

ğŸ‘ï¸ Obstacle Management and Vision System

The PixyCam identifies red and green pillars:

Red pillar: stay on the right side of the lane

Green pillar: stay on the left side of the lane

Using the color recognition data, the robot dynamically adjusts its path.
Obstacle avoidance and wall-following behavior are combined with vision-based decisions for reliable navigation.

ğŸ§ª Testing and Calibration

Several tests were conducted to:

Calibrate ultrasonic sensor distances for accurate wall tracking

Train the PixyCam on various lighting conditions

Tune motor speed and steering PID parameters for smooth cornering

Validate the final three-lap and parking performance

ğŸ“ˆ Results and Performance
Test	Outcome
Track navigation (3 laps)	Completed successfully in consistent times
Traffic sign recognition	>95% accuracy in various lighting conditions
Obstacle avoidance	Smooth navigation with no collisions
Parking maneuver	Successfully performed parallel parking within boundaries
ğŸ¬ Video Demonstration

ğŸ¥ Watch the YouTube Demo

(Ensure the video is set to â€œpublicâ€ or â€œunlistedâ€ so judges can access it)

ğŸ‘¨â€ğŸ”§ Team Members
Name	Role	Responsibility
[Your Name]	Team Leader	System integration, navigation logic
[Member 2]	Hardware Engineer	Chassis design, wiring, and assembly
[Member 3]	Software Engineer	Vision system and code optimization
[Member 4]	Documentation Lead	Report writing and GitHub maintenance
ğŸ“„ License

This project is licensed under the MIT License â€” see the LICENSE
 file for details.
